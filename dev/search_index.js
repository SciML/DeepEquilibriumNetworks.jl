var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"Bai, S.; Kolter, J. Z. and Koltun, V. (2019). Deep Equilibrium Models, arXiv:1909.01377 [cs, stat]. Accessed on Sep 13, 2021, arXiv: 1909.01377.\n\n\n\nBai, S.; Koltun, V. and Kolter, J. Z. (2020). Multiscale Deep Equilibrium Models, arXiv:2006.08656 [cs, stat]. Accessed on Sep 14, 2021, arXiv: 2006.08656.\n\n\n\nPal, A.; Edelman, A. and Rackauckas, C. (2022). Mixing implicit and explicit deep learning with skip DEQs and infinite time neural odes (continuous DEQs). Training 4, 5.\n\n\n\n","category":"page"},{"location":"api/#Deep-Equilibrium-Models","page":"API References","title":"Deep Equilibrium Models","text":"","category":"section"},{"location":"api/","page":"API References","title":"API References","text":"(Bai et al., 2019) introduced Discrete Deep Equilibrium Models which drives a Discrete Dynamical System to its steady-state. (Pal et al., 2022) extends this framework to Continuous Dynamical Systems which converge to the steady-stable in a more stable fashion. For a detailed discussion refer to (Pal et al., 2022).","category":"page"},{"location":"api/","page":"API References","title":"API References","text":"To construct a continuous DEQ, any ODE solver compatible with DifferentialEquations.jl API can be passed as the solver. To construct a discrete DEQ, any root finding algorithm compatible with NonlinearSolve.jl API can be passed as the solver.","category":"page"},{"location":"api/#Choosing-a-Solver","page":"API References","title":"Choosing a Solver","text":"","category":"section"},{"location":"api/#Root-Finding-Algorithms","page":"API References","title":"Root Finding Algorithms","text":"","category":"section"},{"location":"api/","page":"API References","title":"API References","text":"Using Root Finding Algorithms give fast convergence when possible, but these methods also tend to be unstable. If you must use a root finding algorithm, we recommend using:","category":"page"},{"location":"api/","page":"API References","title":"API References","text":"NewtonRaphson or TrustRegion for small models\nLimitedMemoryBroyden for large Deep Learning applications (with well-conditioned Jacobians)\nNewtonRaphson(; linsolve = KrylovJL_GMRES()) for cases when Broyden methods fail","category":"page"},{"location":"api/","page":"API References","title":"API References","text":"Note that Krylov Methods rely on efficient VJPs which are not available for all Lux models. If you think this is causing a performance regression, please open an issue in Lux.jl.","category":"page"},{"location":"api/#ODE-Solvers","page":"API References","title":"ODE Solvers","text":"","category":"section"},{"location":"api/","page":"API References","title":"API References","text":"Using ODE Solvers give slower convergence, but are more stable. We generally recommend these methods over root finding algorithms. If you use implicit ODE solvers, remember to use Krylov linear solvers, see OrdinaryDiffEq.jl documentation for these. For most cases, we recommend:","category":"page"},{"location":"api/","page":"API References","title":"API References","text":"VCAB3() for high tolerance problems\nTsit5() for high tolerance problems where VCAB3() fails\nIn all other cases, follow the recommendation given in OrdinaryDiffEq.jl documentation","category":"page"},{"location":"api/#Sensitivity-Analysis","page":"API References","title":"Sensitivity Analysis","text":"","category":"section"},{"location":"api/","page":"API References","title":"API References","text":"For MultiScaleNeuralODE, we default to GaussAdjoint(; autojacvec = ZygoteVJP()). A faster alternative would be BacksolveAdjoint(; autojacvec = ZygoteVJP()) but there are stability concerns for using that. Follow the recommendation given in SciMLSensitivity.jl documentation.\nFor Steady State Problems, we default to SteadyStateAdjoint(; linsolve = SimpleGMRES(; blocksize, linsolve_kwargs = (; maxiters=10, abstol=1e-3, reltol=1e-3))). This default will perform poorly on small models. It is recommended to pass sensealg = SteadyStateAdjoint() or sensealg = SteadyStateAdjoint(; linsolve = LUFactorization()) for small models.","category":"page"},{"location":"api/#Standard-Models","page":"API References","title":"Standard Models","text":"","category":"section"},{"location":"api/","page":"API References","title":"API References","text":"DeepEquilibriumNetwork\nSkipDeepEquilibriumNetwork","category":"page"},{"location":"api/#DeepEquilibriumNetworks.DeepEquilibriumNetwork","page":"API References","title":"DeepEquilibriumNetworks.DeepEquilibriumNetwork","text":"DeepEquilibriumNetwork(model, solver; init = missing, jacobian_regularization=nothing,\n    problem_type::Type{pType}=SteadyStateProblem{false}, kwargs...)\n\nDeep Equilibrium Network as proposed in (Bai et al., 2019) and (Pal et al., 2022).\n\nArguments\n\nmodel: Neural Network.\nsolver: Solver for the rootfinding problem. ODE Solvers and Nonlinear Solvers are both supported.\n\nKeyword Arguments\n\ninit: Initial Condition for the rootfinding problem. If nothing, the initial condition is set to zero(x). If missing, the initial condition is set to WrappedFunction(zero). In other cases the initial condition is set to init(x, ps, st).\njacobian_regularization: Must be one of nothing, AutoForwardDiff, AutoFiniteDiff or AutoZygote.\nproblem_type: Provides a way to simulate a Vanilla Neural ODE by setting the problem_type to ODEProblem. By default, the problem type is set to SteadyStateProblem.\nkwargs: Additional Parameters that are directly passed to SciMLBase.solve.\n\nExample\n\njulia> model = DeepEquilibriumNetwork(\n           Parallel(+, Dense(2, 2; use_bias=false), Dense(2, 2; use_bias=false)),\n           VCABM3(); verbose=false)\nDeepEquilibriumNetwork(\n    model = Parallel(\n        +\n        Dense(2 => 2, bias=false),      # 4 parameters\n        Dense(2 => 2, bias=false),      # 4 parameters\n    ),\n    init = WrappedFunction(Base.Fix1{typeof(DeepEquilibriumNetworks.__zeros_init), Nothing}(DeepEquilibriumNetworks.__zeros_init, nothing)),\n)         # Total: 8 parameters,\n          #        plus 0 states.\n\njulia> rng = Xoshiro(0);\n\njulia> ps, st = Lux.setup(rng, model);\n\njulia> size(first(model(ones(Float32, 2, 1), ps, st)))\n(2, 1)\n\nSee also: SkipDeepEquilibriumNetwork, MultiScaleDeepEquilibriumNetwork, MultiScaleSkipDeepEquilibriumNetwork.\n\n\n\n\n\n","category":"type"},{"location":"api/#DeepEquilibriumNetworks.SkipDeepEquilibriumNetwork","page":"API References","title":"DeepEquilibriumNetworks.SkipDeepEquilibriumNetwork","text":"SkipDeepEquilibriumNetwork(model, [init=nothing,] solver; kwargs...)\n\nSkip Deep Equilibrium Network as proposed in (Pal et al., 2022). Alias which creates a DeepEquilibriumNetwork with init kwarg set to passed value.\n\n\n\n\n\n","category":"function"},{"location":"api/#MultiScale-Models","page":"API References","title":"MultiScale Models","text":"","category":"section"},{"location":"api/","page":"API References","title":"API References","text":"MultiScaleDeepEquilibriumNetwork\nMultiScaleSkipDeepEquilibriumNetwork\nMultiScaleNeuralODE","category":"page"},{"location":"api/#DeepEquilibriumNetworks.MultiScaleDeepEquilibriumNetwork","page":"API References","title":"DeepEquilibriumNetworks.MultiScaleDeepEquilibriumNetwork","text":"MultiScaleDeepEquilibriumNetwork(main_layers::Tuple, mapping_layers::Matrix,\n    post_fuse_layer::Union{Nothing, Tuple}, solver,\n    scales::NTuple{N, NTuple{L, Int64}}; kwargs...)\n\nMulti Scale Deep Equilibrium Network as proposed in (Bai et al., 2020).\n\nArguments\n\nmain_layers: Tuple of Neural Networks. Each Neural Network is applied to the corresponding scale.\nmapping_layers: Matrix of Neural Networks. Each Neural Network is applied to the corresponding scale and the corresponding layer.\npost_fuse_layer: Neural Network applied to the fused output of the main layers.\nsolver: Solver for the rootfinding problem. ODE Solvers and Nonlinear Solvers are both supported.\nscales: Scales of the Multi Scale DEQ. Each scale is a tuple of integers. The length of the tuple is the number of layers in the corresponding main layer.\n\nFor keyword arguments, see DeepEquilibriumNetwork.\n\nExample\n\njulia> main_layers = (\n           Parallel(+, Dense(4 => 4, tanh; use_bias=false), Dense(4 => 4, tanh; use_bias=false)),\n           Dense(3 => 3, tanh), Dense(2 => 2, tanh), Dense(1 => 1, tanh));\n\njulia> mapping_layers = [NoOpLayer() Dense(4 => 3, tanh) Dense(4 => 2, tanh) Dense(4 => 1, tanh);\n                         Dense(3 => 4, tanh) NoOpLayer() Dense(3 => 2, tanh) Dense(3 => 1, tanh);\n                         Dense(2 => 4, tanh) Dense(2 => 3, tanh) NoOpLayer() Dense(2 => 1, tanh);\n                         Dense(1 => 4, tanh) Dense(1 => 3, tanh) Dense(1 => 2, tanh) NoOpLayer()];\n\njulia> model = MultiScaleDeepEquilibriumNetwork(\n           main_layers, mapping_layers, nothing, NewtonRaphson(), ((4,), (3,), (2,), (1,)));\n\njulia> rng = Xoshiro(0);\n\njulia> ps, st = Lux.setup(rng, model);\n\njulia> x = rand(rng, Float32, 4, 12);\n\njulia> size.(first(model(x, ps, st)))\n((4, 12), (3, 12), (2, 12), (1, 12))\n\n\n\n\n\n","category":"function"},{"location":"api/#DeepEquilibriumNetworks.MultiScaleSkipDeepEquilibriumNetwork","page":"API References","title":"DeepEquilibriumNetworks.MultiScaleSkipDeepEquilibriumNetwork","text":"MultiScaleSkipDeepEquilibriumNetwork(main_layers::Tuple, mapping_layers::Matrix,\n    post_fuse_layer::Union{Nothing, Tuple}, [init = nothing,] solver,\n    scales::NTuple{N, NTuple{L, Int64}}; kwargs...)\n\nSkip Multi Scale Deep Equilibrium Network as proposed in (Pal et al., 2022). Alias which creates a MultiScaleDeepEquilibriumNetwork with init kwarg set to passed value.\n\nIf init is not passed, it creates a MultiScale Regularized Deep Equilibrium Network.\n\n\n\n\n\n","category":"function"},{"location":"api/#DeepEquilibriumNetworks.MultiScaleNeuralODE","page":"API References","title":"DeepEquilibriumNetworks.MultiScaleNeuralODE","text":"MultiScaleNeuralODE(args...; kwargs...)\n\nSame arguments as MultiScaleDeepEquilibriumNetwork but sets problem_type to ODEProblem{false}.\n\n\n\n\n\n","category":"function"},{"location":"api/#Solution","page":"API References","title":"Solution","text":"","category":"section"},{"location":"api/","page":"API References","title":"API References","text":"DeepEquilibriumSolution","category":"page"},{"location":"api/#DeepEquilibriumNetworks.DeepEquilibriumSolution","page":"API References","title":"DeepEquilibriumNetworks.DeepEquilibriumSolution","text":"DeepEquilibriumSolution(z_star, u₀, residual, jacobian_loss, nfe, solution)\n\nStores the solution of a DeepEquilibriumNetwork and its variants.\n\nFields\n\nz_star: Steady-State or the value reached due to maxiters\nu0: Initial Condition\nresidual: Difference of the z^* and f(z^* x)\njacobian_loss: Jacobian Stabilization Loss (see individual networks to see how it can be computed)\nnfe: Number of Function Evaluations\noriginal: Original Internal Solution\n\n\n\n\n\n","category":"type"},{"location":"tutorials/reduced_dim_deq/#Modelling-Equilibrium-Models-with-Reduced-State-Size","page":"Modelling Equilibrium Models with Reduced State Size","title":"Modelling Equilibrium Models with Reduced State Size","text":"","category":"section"},{"location":"tutorials/reduced_dim_deq/","page":"Modelling Equilibrium Models with Reduced State Size","title":"Modelling Equilibrium Models with Reduced State Size","text":"Sometimes we want don't want to solve a root finding problem with the full state size. This will often be faster, since the size of the root finding problem is reduced. We will use the same MNIST example as before, but this time we will use a reduced state size.","category":"page"},{"location":"tutorials/reduced_dim_deq/","page":"Modelling Equilibrium Models with Reduced State Size","title":"Modelling Equilibrium Models with Reduced State Size","text":"using DeepEquilibriumNetworks, SciMLSensitivity, Lux, NonlinearSolve, OrdinaryDiffEq,\n      Statistics, Random, Optimisers, LuxCUDA, Zygote, LinearSolve, Dates, Printf\nusing MLDatasets: MNIST\nusing MLDataUtils: LabelEnc, convertlabel, stratifiedobs, batchview\n\nCUDA.allowscalar(false)\nENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true\n\nconst cdev = cpu_device()\nconst gdev = gpu_device()\n\nfunction onehot(labels_raw)\n    return convertlabel(LabelEnc.OneOfK, labels_raw, LabelEnc.NativeLabels(collect(0:9)))\nend\n\nfunction loadmnist(batchsize, split)\n    # Load MNIST\n    mnist = MNIST(; split)\n    imgs, labels_raw = mnist.features, mnist.targets\n    # Process images into (H,W,C,BS) batches\n    x_train = Float32.(reshape(imgs, size(imgs, 1), size(imgs, 2), 1, size(imgs, 3))) |>\n              gdev\n    x_train = batchview(x_train, batchsize)\n    # Onehot and batch the labels\n    y_train = onehot(labels_raw) |> gdev\n    y_train = batchview(y_train, batchsize)\n    return x_train, y_train\nend\n\nx_train, y_train = loadmnist(128, :train);\nx_test, y_test = loadmnist(128, :test);","category":"page"},{"location":"tutorials/reduced_dim_deq/","page":"Modelling Equilibrium Models with Reduced State Size","title":"Modelling Equilibrium Models with Reduced State Size","text":"Now we will define the construct model function. Here we will use Dense Layers and downsample the features using the init kwarg.","category":"page"},{"location":"tutorials/reduced_dim_deq/","page":"Modelling Equilibrium Models with Reduced State Size","title":"Modelling Equilibrium Models with Reduced State Size","text":"function construct_model(solver; model_type::Symbol=:regdeq)\n    down = Chain(FlattenLayer(), Dense(784 => 512, gelu))\n\n    # The input layer of the DEQ\n    deq_model = Chain(Parallel(+, Dense(128 => 64, tanh),   # Reduced dim of `128`\n            Dense(512 => 64, tanh)),  # Original dim of `512`\n        Dense(64 => 64, tanh), Dense(64 => 128))       # Return the reduced dim of `128`\n\n    if model_type === :skipdeq\n        init = Dense(512 => 128, tanh)\n    elseif model_type === :regdeq\n        error(\":regdeq is not supported for reduced dim models\")\n    else\n        # This should preferably done via `ChainRulesCore.@ignore_derivatives`. But here\n        # we are only using Zygote so this is fine.\n        init = WrappedFunction(x -> Zygote.@ignore(fill!(\n            similar(x, 128, size(x, 2)), false)))\n    end\n\n    deq = DeepEquilibriumNetwork(\n        deq_model, solver; init, verbose=false, linsolve_kwargs=(; maxiters=10))\n\n    classifier = Chain(Dense(128 => 128, gelu), Dense(128, 10))\n\n    model = Chain(; down, deq, classifier)\n\n    # For NVIDIA GPUs this directly generates the parameters on the GPU\n    rng = Random.default_rng() |> gdev\n    ps, st = Lux.setup(rng, model)\n\n    # Warmup the forward and backward passes\n    x = randn(rng, Float32, 28, 28, 1, 128)\n    y = onehot(rand(Random.default_rng(), 0:9, 128)) |> gdev\n\n    model_ = StatefulLuxLayer(model, ps, st)\n    @printf \"[%s] warming up forward pass\\n\" string(now())\n    logitcrossentropy(model_, x, ps, y)\n    @printf \"[%s] warming up backward pass\\n\" string(now())\n    Zygote.gradient(logitcrossentropy, model_, x, ps, y)\n    @printf \"[%s] warmup complete\\n\" string(now())\n\n    return model, ps, st\nend","category":"page"},{"location":"tutorials/reduced_dim_deq/","page":"Modelling Equilibrium Models with Reduced State Size","title":"Modelling Equilibrium Models with Reduced State Size","text":"Define some helper functions to train the model.","category":"page"},{"location":"tutorials/reduced_dim_deq/","page":"Modelling Equilibrium Models with Reduced State Size","title":"Modelling Equilibrium Models with Reduced State Size","text":"logitcrossentropy(ŷ, y) = mean(-sum(y .* logsoftmax(ŷ; dims=1); dims=1))\nfunction logitcrossentropy(model, x, ps, y)\n    l1 = logitcrossentropy(model(x, ps), y)\n    # Add in some regularization\n    l2 = mean(abs2, model.st.deq.solution.z_star .- model.st.deq.solution.u0)\n    return l1 + 0.1f0 * l2\nend\n\nclassify(x) = argmax.(eachcol(x))\n\nfunction accuracy(model, data, ps, st)\n    total_correct, total = 0, 0\n    st = Lux.testmode(st)\n    model = StatefulLuxLayer(model, ps, st)\n    for (x, y) in data\n        target_class = classify(cdev(y))\n        predicted_class = classify(cdev(model(x)))\n        total_correct += sum(target_class .== predicted_class)\n        total += length(target_class)\n    end\n    return total_correct / total\nend\n\nfunction train_model(\n        solver, model_type; data_train=zip(x_train, y_train), data_test=zip(x_test, y_test))\n    model, ps, st = construct_model(solver; model_type)\n    model_st = StatefulLuxLayer(model, nothing, st)\n\n    @printf \"[%s] Training Model: %s with Solver: %s\\n\" string(now()) model_type nameof(typeof(solver))\n\n    opt_st = Optimisers.setup(Adam(0.001), ps)\n\n    acc = accuracy(model, data_test, ps, st) * 100\n    @printf \"[%s] Starting Accuracy: %.5f%%\\n\" string(now()) acc\n\n    @printf \"[%s] Pretrain with unrolling to a depth of 5\\n\" string(now())\n    st = Lux.update_state(st, :fixed_depth, Val(5))\n    model_st = StatefulLuxLayer(model, ps, st)\n\n    for (i, (x, y)) in enumerate(data_train)\n        res = Zygote.withgradient(logitcrossentropy, model_st, x, ps, y)\n        Optimisers.update!(opt_st, ps, res.grad[3])\n        i % 50 == 1 &&\n            @printf \"[%s] Pretraining Batch: [%4d/%4d] Loss: %.5f\\n\" string(now()) i length(data_train) res.val\n    end\n\n    acc = accuracy(model, data_test, ps, model_st.st) * 100\n    @printf \"[%s] Pretraining complete. Accuracy: %.5f%%\\n\" string(now()) acc\n\n    st = Lux.update_state(st, :fixed_depth, Val(0))\n    model_st = StatefulLuxLayer(model, ps, st)\n\n    for epoch in 1:3\n        for (i, (x, y)) in enumerate(data_train)\n            res = Zygote.withgradient(logitcrossentropy, model_st, x, ps, y)\n            Optimisers.update!(opt_st, ps, res.grad[3])\n            i % 50 == 1 &&\n                @printf \"[%s] Epoch: [%d/%d] Batch: [%4d/%4d] Loss: %.5f\\n\" string(now()) epoch 3 i length(data_train) res.val\n        end\n\n        acc = accuracy(model, data_test, ps, model_st.st) * 100\n        @printf \"[%s] Epoch: [%d/%d] Accuracy: %.5f%%\\n\" string(now()) epoch 3 acc\n    end\n\n    @printf \"[%s] Training complete.\\n\" string(now())\n\n    return model, ps, st\nend","category":"page"},{"location":"tutorials/reduced_dim_deq/","page":"Modelling Equilibrium Models with Reduced State Size","title":"Modelling Equilibrium Models with Reduced State Size","text":"Now we can train our model. We can't use :regdeq here currently, but we will support this in the future.","category":"page"},{"location":"tutorials/reduced_dim_deq/","page":"Modelling Equilibrium Models with Reduced State Size","title":"Modelling Equilibrium Models with Reduced State Size","text":"train_model(NewtonRaphson(; linsolve=KrylovJL_GMRES()), :skipdeq)\nnothing # hide","category":"page"},{"location":"tutorials/reduced_dim_deq/","page":"Modelling Equilibrium Models with Reduced State Size","title":"Modelling Equilibrium Models with Reduced State Size","text":"train_model(NewtonRaphson(; linsolve=KrylovJL_GMRES()), :deq)\nnothing # hide","category":"page"},{"location":"tutorials/basic_mnist_deq/#Training-a-Simple-MNIST-Classifier-using-Deep-Equilibrium-Models","page":"Training a Simple MNIST Classifier using Deep Equilibrium Models","title":"Training a Simple MNIST Classifier using Deep Equilibrium Models","text":"","category":"section"},{"location":"tutorials/basic_mnist_deq/","page":"Training a Simple MNIST Classifier using Deep Equilibrium Models","title":"Training a Simple MNIST Classifier using Deep Equilibrium Models","text":"We will train a simple Deep Equilibrium Model on MNIST. First we load a few packages.","category":"page"},{"location":"tutorials/basic_mnist_deq/","page":"Training a Simple MNIST Classifier using Deep Equilibrium Models","title":"Training a Simple MNIST Classifier using Deep Equilibrium Models","text":"using DeepEquilibriumNetworks, SciMLSensitivity, Lux, NonlinearSolve, OrdinaryDiffEq,\n      Statistics, Random, Optimisers, LuxCUDA, Zygote, LinearSolve, Dates, Printf\nusing MLDatasets: MNIST\nusing MLDataUtils: LabelEnc, convertlabel, stratifiedobs, batchview\n\nCUDA.allowscalar(false)\nENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true","category":"page"},{"location":"tutorials/basic_mnist_deq/","page":"Training a Simple MNIST Classifier using Deep Equilibrium Models","title":"Training a Simple MNIST Classifier using Deep Equilibrium Models","text":"Setup device functions from Lux. See GPU Management for more details.","category":"page"},{"location":"tutorials/basic_mnist_deq/","page":"Training a Simple MNIST Classifier using Deep Equilibrium Models","title":"Training a Simple MNIST Classifier using Deep Equilibrium Models","text":"const cdev = cpu_device()\nconst gdev = gpu_device()","category":"page"},{"location":"tutorials/basic_mnist_deq/","page":"Training a Simple MNIST Classifier using Deep Equilibrium Models","title":"Training a Simple MNIST Classifier using Deep Equilibrium Models","text":"We can now construct our dataloader. We are using only limited part of the data for demonstration.","category":"page"},{"location":"tutorials/basic_mnist_deq/","page":"Training a Simple MNIST Classifier using Deep Equilibrium Models","title":"Training a Simple MNIST Classifier using Deep Equilibrium Models","text":"function onehot(labels_raw)\n    return convertlabel(LabelEnc.OneOfK, labels_raw, LabelEnc.NativeLabels(collect(0:9)))\nend\n\nfunction loadmnist(batchsize, split)\n    # Load MNIST\n    mnist = MNIST(; split)\n    imgs, labels_raw = mnist.features, mnist.targets\n    # Process images into (H,W,C,BS) batches\n    x_train = Float32.(reshape(imgs, size(imgs, 1), size(imgs, 2), 1, size(imgs, 3)))[\n        :, :, 1:1, 1:128] |> gdev\n    x_train = batchview(x_train, batchsize)\n    # Onehot and batch the labels\n    y_train = onehot(labels_raw)[:, 1:128] |> gdev\n    y_train = batchview(y_train, batchsize)\n    return x_train, y_train\nend\n\nx_train, y_train = loadmnist(16, :train);\nx_test, y_test = loadmnist(16, :test);","category":"page"},{"location":"tutorials/basic_mnist_deq/","page":"Training a Simple MNIST Classifier using Deep Equilibrium Models","title":"Training a Simple MNIST Classifier using Deep Equilibrium Models","text":"Construct the Lux Neural Network containing a DEQ layer.","category":"page"},{"location":"tutorials/basic_mnist_deq/","page":"Training a Simple MNIST Classifier using Deep Equilibrium Models","title":"Training a Simple MNIST Classifier using Deep Equilibrium Models","text":"function construct_model(solver; model_type::Symbol=:deq)\n    down = Chain(Conv((3, 3), 1 => 64, gelu; stride=1), GroupNorm(64, 64),\n        Conv((4, 4), 64 => 64; stride=2, pad=1))\n\n    # The input layer of the DEQ\n    deq_model = Chain(\n        Parallel(+, Conv((3, 3), 64 => 64, tanh; stride=1, pad=SamePad()),\n            Conv((3, 3), 64 => 64, tanh; stride=1, pad=SamePad())),\n        Conv((3, 3), 64 => 64, tanh; stride=1, pad=SamePad()))\n\n    if model_type === :skipdeq\n        init = Conv((3, 3), 64 => 64, gelu; stride=1, pad=SamePad())\n    elseif model_type === :regdeq\n        init = nothing\n    else\n        init = missing\n    end\n\n    deq = DeepEquilibriumNetwork(\n        deq_model, solver; init, verbose=false, linsolve_kwargs=(; maxiters=10))\n\n    classifier = Chain(\n        GroupNorm(64, 64, relu), GlobalMeanPool(), FlattenLayer(), Dense(64, 10))\n\n    model = Chain(; down, deq, classifier)\n\n    # For NVIDIA GPUs this directly generates the parameters on the GPU\n    rng = Random.default_rng() |> gdev\n    ps, st = Lux.setup(rng, model)\n\n    # Warmup the forward and backward passes\n    x = randn(rng, Float32, 28, 28, 1, 128)\n    y = onehot(rand(Random.default_rng(), 0:9, 128)) |> gdev\n\n    model_ = StatefulLuxLayer(model, ps, st)\n    @printf \"[%s] warming up forward pass\\n\" string(now())\n    logitcrossentropy(model_, x, ps, y)\n    @printf \"[%s] warming up backward pass\\n\" string(now())\n    Zygote.gradient(logitcrossentropy, model_, x, ps, y)\n    @printf \"[%s] warmup complete\\n\" string(now())\n\n    return model, ps, st\nend","category":"page"},{"location":"tutorials/basic_mnist_deq/","page":"Training a Simple MNIST Classifier using Deep Equilibrium Models","title":"Training a Simple MNIST Classifier using Deep Equilibrium Models","text":"Define some helper functions to train the model.","category":"page"},{"location":"tutorials/basic_mnist_deq/","page":"Training a Simple MNIST Classifier using Deep Equilibrium Models","title":"Training a Simple MNIST Classifier using Deep Equilibrium Models","text":"logitcrossentropy(ŷ, y) = mean(-sum(y .* logsoftmax(ŷ; dims=1); dims=1))\nfunction logitcrossentropy(model, x, ps, y)\n    l1 = logitcrossentropy(model(x, ps), y)\n    # Add in some regularization\n    l2 = mean(abs2, model.st.deq.solution.z_star .- model.st.deq.solution.u0)\n    return l1 + 10.0 * l2\nend\n\nclassify(x) = argmax.(eachcol(x))\n\nfunction accuracy(model, data, ps, st)\n    total_correct, total = 0, 0\n    st = Lux.testmode(st)\n    model = StatefulLuxLayer(model, ps, st)\n    for (x, y) in data\n        target_class = classify(cdev(y))\n        predicted_class = classify(cdev(model(x)))\n        total_correct += sum(target_class .== predicted_class)\n        total += length(target_class)\n    end\n    return total_correct / total\nend\n\nfunction train_model(\n        solver, model_type; data_train=zip(x_train, y_train), data_test=zip(x_test, y_test))\n    model, ps, st = construct_model(solver; model_type)\n    model_st = StatefulLuxLayer(model, nothing, st)\n\n    @printf \"[%s] Training Model: %s with Solver: %s\\n\" string(now()) model_type nameof(typeof(solver))\n\n    opt_st = Optimisers.setup(Adam(0.001), ps)\n\n    acc = accuracy(model, data_test, ps, st) * 100\n    @printf \"[%s] Starting Accuracy: %.5f%%\\n\" string(now()) acc\n\n    @printf \"[%s] Pretrain with unrolling to a depth of 5\\n\" string(now())\n    st = Lux.update_state(st, :fixed_depth, Val(5))\n    model_st = StatefulLuxLayer(model, ps, st)\n\n    for (i, (x, y)) in enumerate(data_train)\n        res = Zygote.withgradient(logitcrossentropy, model_st, x, ps, y)\n        Optimisers.update!(opt_st, ps, res.grad[3])\n        i % 50 == 1 &&\n            @printf \"[%s] Pretraining Batch: [%4d/%4d] Loss: %.5f\\n\" string(now()) i length(data_train) res.val\n    end\n\n    acc = accuracy(model, data_test, ps, model_st.st) * 100\n    @printf \"[%s] Pretraining complete. Accuracy: %.5f%%\\n\" string(now()) acc\n\n    st = Lux.update_state(st, :fixed_depth, Val(0))\n    model_st = StatefulLuxLayer(model, ps, st)\n\n    for epoch in 1:3\n        for (i, (x, y)) in enumerate(data_train)\n            res = Zygote.withgradient(logitcrossentropy, model_st, x, ps, y)\n            Optimisers.update!(opt_st, ps, res.grad[3])\n            i % 50 == 1 &&\n                @printf \"[%s] Epoch: [%d/%d] Batch: [%4d/%4d] Loss: %.5f\\n\" string(now()) epoch 3 i length(data_train) res.val\n        end\n\n        acc = accuracy(model, data_test, ps, model_st.st) * 100\n        @printf \"[%s] Epoch: [%d/%d] Accuracy: %.5f%%\\n\" string(now()) epoch 3 acc\n    end\n\n    @printf \"[%s] Training complete.\\n\" string(now())\n\n    return model, ps, st\nend","category":"page"},{"location":"tutorials/basic_mnist_deq/","page":"Training a Simple MNIST Classifier using Deep Equilibrium Models","title":"Training a Simple MNIST Classifier using Deep Equilibrium Models","text":"Now we can train our model. First we will train a Discrete DEQ, which effectively means pass in a root finding algorithm. Typically most packages lack good nonlinear solvers, and end up using solvers like Broyden, but we can simply slap in any of the fancy solvers from NonlinearSolve.jl. Here we will use Newton-Krylov Method:","category":"page"},{"location":"tutorials/basic_mnist_deq/","page":"Training a Simple MNIST Classifier using Deep Equilibrium Models","title":"Training a Simple MNIST Classifier using Deep Equilibrium Models","text":"train_model(NewtonRaphson(; linsolve=KrylovJL_GMRES()), :regdeq);\nnothing # hide","category":"page"},{"location":"tutorials/basic_mnist_deq/","page":"Training a Simple MNIST Classifier using Deep Equilibrium Models","title":"Training a Simple MNIST Classifier using Deep Equilibrium Models","text":"We can also train a continuous DEQ by passing in an ODE solver. Here we will use VCAB3() which tend to be quite fast for continuous Neural Network problems.","category":"page"},{"location":"tutorials/basic_mnist_deq/","page":"Training a Simple MNIST Classifier using Deep Equilibrium Models","title":"Training a Simple MNIST Classifier using Deep Equilibrium Models","text":"train_model(VCAB3(), :deq);\nnothing # hide","category":"page"},{"location":"tutorials/basic_mnist_deq/","page":"Training a Simple MNIST Classifier using Deep Equilibrium Models","title":"Training a Simple MNIST Classifier using Deep Equilibrium Models","text":"This code is setup to allow playing around with different DEQ models. Try modifying the model_type argument to train_model to :skipdeq or :deq to see how the model behaves. You can also try different solvers from NonlinearSolve.jl and OrdinaryDiffEq.jl! Even 3rd party solvers from Sundials.jl will work, just remember to use CPU for those.","category":"page"},{"location":"#DeepEquilibriumNetworks.jl","page":"Home","title":"DeepEquilibriumNetworks.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"DeepEquilibriumNetworks.jl is a framework built on top of DifferentialEquations.jl and Lux.jl, enabling the efficient training and inference for Deep Equilibrium Networks (Infinitely Deep Neural Networks).","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install DeepEquilibriumNetworks.jl, use the Julia package manager:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"DeepEquilibriumNetworks\")","category":"page"},{"location":"#Quick-start","page":"Home","title":"Quick-start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using DeepEquilibriumNetworks, Lux, Random, NonlinearSolve, Zygote, SciMLSensitivity\nusing LuxCUDA  # For NVIDIA GPU support\n\nseed = 0\nrng = Random.default_rng()\nRandom.seed!(rng, seed)\n\nmodel = Chain(Dense(2 => 2),\n    DeepEquilibriumNetwork(\n        Parallel(+, Dense(2 => 2; use_bias=false), Dense(2 => 2; use_bias=false)),\n        NewtonRaphson()))\n\ngdev = gpu_device()\ncdev = cpu_device()\n\nps, st = Lux.setup(rng, model) |> gdev\nx = rand(rng, Float32, 2, 3) |> gdev\ny = rand(rng, Float32, 2, 3) |> gdev\n\nres, st_ = model(x, ps, st)\nst_.layer_2.solution","category":"page"},{"location":"","page":"Home","title":"Home","text":"gs = only(Zygote.gradient(p -> sum(abs2, first(model(x, p, st)) .- y), ps))","category":"page"},{"location":"#Citation","page":"Home","title":"Citation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you are using this project for research or other academic purposes, consider citing our paper:","category":"page"},{"location":"","page":"Home","title":"Home","text":"@article{pal2022continuous,\n  title={Continuous Deep Equilibrium Models: Training Neural ODEs Faster by Integrating Them to Infinity},\n  author={Pal, Avik and Edelman, Alan and Rackauckas, Christopher},\n  booktitle={2023 IEEE High Performance Extreme Computing Conference (HPEC)}, \n  year={2023}\n}","category":"page"},{"location":"","page":"Home","title":"Home","text":"For specific algorithms, check the respective documentations and cite the corresponding papers.","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nSee the SciML Style Guide for common coding practices and other style decisions.\nThere are a few community forums:\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Zulip\nOn the Julia Discourse forums\nSee also SciML Community page","category":"page"},{"location":"#Reproducibility","page":"Home","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"</details>","category":"page"},{"location":"","page":"Home","title":"Home","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"Home","title":"Home","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"</details>","category":"page"},{"location":"","page":"Home","title":"Home","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg # hide\nPkg.status(; mode=PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"</details>","category":"page"},{"location":"","page":"Home","title":"Home","text":"using TOML\nusing Markdown\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink_manifest = \"https://github.com/SciML/\" *\n                name *\n                \".jl/tree/gh-pages/v\" *\n                version *\n                \"/assets/Manifest.toml\"\nlink_project = \"https://github.com/SciML/\" *\n               name *\n               \".jl/tree/gh-pages/v\" *\n               version *\n               \"/assets/Project.toml\"\nMarkdown.parse(\"\"\"You can also download the\n[manifest]($link_manifest)\nfile and the\n[project]($link_project)\nfile.\n\"\"\")","category":"page"}]
}
